{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stylegan.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNCDZkJwoYtLw18VmH+E7F7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-txPcpH-ZFs6"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy\n","import torchvision"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-9XlN5Mnd2Tk"},"source":["# Model Implementation"]},{"cell_type":"code","metadata":{"id":"SiQ6CkVhZghm"},"source":["# c : content\n","# s : style\n","# content 에서 content의 특징을 빼주고 style의 특징을 더해준다.\n","def AdaIN(c, s):\n","  c_mean, c_std = torch.mean(x, dim=1), torch.std(x, dim=1)\n","  s_mean, s_std = torch.mean(y, dim=1), torch.std(y, dim=1)\n","\n","  return s_std * ((c-c_mean)/c_std) + s_mean\n","  \n","# pixelwise feature vector normalization in generator\n","# generator와 discriminator의 magnitude가 competition에 의해 통제 불능의 상태가 되는 것을 막기 위해\n","# 각 convolution의 feature에 대해 pixel 단위로 normalizing을 해준다.\n","# 결과 품질에는 큰 영향이 없으나 필요시 signal magnitude가 점차 증가되는 것을 막아준다.\n","class PixelNorm(nn.Module):\n","  def __init__(self, epsilon=1e-8):\n","    super(PixelNorm, self).__init__()\n","    self.epsilon=epsilon\n","  \n","  def forward(self, x):\n","    return x * torch.rsqrt(torch.mean(torch.square(x), dim=1, keepdim=True) + self.epsilon)\n","\n","# ProGAN(PGGAN, PGAN)에서의 equalized linaer layer\n","class EqualizedLinear(nn.Module):\n","  def __init__(self,in_dim, out_dim, bias=True, bias_init=0, c=1):\n","    super(EqualizedLinear, self).__init__()\n","    self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div(c))\n","\n","    if bias:\n","      self.bias = nn.Parameter(torch.randn(out_dim).fill_(bias_init))\n","    else:\n","      self.bias = None\n","    \n","    self.scale = (1/math.sqrt(in_dim)) * c\n","    self.c = c\n","\n","  def forward(self, x):\n","    out = F.linear(x, self.weight * self.scale)\n","    out = F.leaky_relu(out, negative_slope=0.2)\n","\n","    return out\n","\n","# Synthesis input의 크기가 고정이다.\n","# 4x4x512 크기인 tensor를 생성\n","class ConstInput(nn.Module):\n","  def __init__(self, channel=512, size=4):\n","    super(ConstInput, self).__init__()\n","    self.input = nn.Parameter(torch.randn(1, channel, size, size))\n","  \n","  def forward(self,batch_size):\n","    x = self.input.repeat(batch_size, 1, 1, 1)\n","    return x\n","\n","\n","class SGConv2d(nn.Module):\n","  def __init__(self, in_channel, out_channel, kernel_size=3, style_dim=512, demodulate=True, upsample=False, blur_filter=[1,3,3,1]):\n","    super(SGConv2d, self).__init__()\n","    self.eps = 1e-8\n","    self.kernel_size= kernel_size\n","    self.in_ch = in_channel\n","    self.out_ch = out_channel\n","    self.upsample = upsample\n","    \n","    if upsample:\n","      self.up = nn.Upsample(scale_factor=2, mode=linear)\n","\n","    fan_in = in_channel * kernel_size**2\n","    self.scale = 1 / math.sqrt(fan_in)\n","    self.padding = kernel_size // 2\n","\n","    self.weight = nn.Parameter(torch.randn(1, out_cahnnel, in_channel, kernel_size, kernel_size))\n","\n","    self.modulation = EqualizedLinear(style_dim, in_channel, bias_init=1)\n","    self.demodulate = demodulate\n","\n","  def forward(self, x, style):\n","    batch, _, height, width = x.size()\n","\n","    style = self.modulation(style).view(batch, 1, self.in_ch, 1, 1)\n","    weight = self.scale * self.weight * style\n","\n","    if self.demodulate:\n","      demod = torch.rsqrt(weight.pow(2).sum([2,3,4])+self.epsilon)\n","      weight = weight * demod.view(batch, self.out_ch, 1, 1, 1)\n","    \n","    weight = weight.view(batch*self.out_ch, self.in_ch, self.kernel_size, self.kernel_size)\n","\n","    if self.upsample:\n","      x = x.view(1, batch*self.in_ch, height, width)\n","      weight = weight.view(batch, self.out_ch, self.in_ch, self.kernel_size, self.kernel_size)\n","      weight = weifht.transpose(1, 2).view(batch*self.in_ch, self.out_ch, self.kernel_size, self.kernel_size)\n","\n","      self.up(x)\n","      out = F.conv_transpose2d(x, weight, padding=0, stride=2, groups=batch)\n","      _,_, height, width = out.size()\n","\n","      out = out.view(batch, self.out_ch, height, width)\n","    \n","\n","\n","\n","\n","class StyleGAN(nn.Module):\n","  def __init__(self,):\n","    super(StyleGAN, self).__init__()\n","\n","    # Mapping Network ---------------------------------------------\n","    self.layers = []\n","    self.layers.append(PixelNorm())\n","    for idx in range(8):\n","      self.layers.append(EqualizedLinear(latent_dim, latent_dim))\n","      self.layers.append(nn.LeakyReLU(0.2))\n","    \n","    # style\n","    self.mapping = nn.Sequential(*self.layers)\n","    # -------------------------------------------------------------\n","\n","    # Synthesis Network -------------------------------------------\n","    blur_filter = [1,3,3,1]\n","    channel_multiplier = 2\n","    self.channels = {\n","      4: 512,\n","      8: 512,\n","      16: 512,\n","      32: 512,\n","      64: 256 * channel_multiplier,\n","      128: 128 * channel_multiplier,\n","      256: 64 * channel_multiplier,\n","      512: 32 * channel_multiplier,\n","      1024: 16 * channel_multiplier,\n","    }\n","\n","    self.constInput = ConstInput()\n","\n","    \n","    # -------------------------------------------------------------\n","\n","  def forward(self, z, label):\n","    # Mapping Network ---------------------------------------------\n","    label = torch.dot(label, z.float())\n","\n","    x = torch.concat([z, label])\n","    w = self.map(x)\n","    w = w.view(-1, 1)\n","    # -------------------------------------------------------------\n","\n","    # Synthesis Network -------------------------------------------\n","\n","    return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtRJvSHXtksY"},"source":[""],"execution_count":null,"outputs":[]}]}